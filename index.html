<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>AR Throw Predictor – Fixed Start</title>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose/pose.js"></script>

<style>
html, body {
  margin:0;
  padding:0;
  width:100%;
  height:100%;
  background:black;
  overflow:hidden;
}

#camera-container {
  position:absolute;
  top:0;
  left:0;
  width:100%;
  height:100%;
  z-index:0;
}

video, canvas {
  position:absolute;
  top:0;
  left:0;
  width:100%;
  height:100%;
  object-fit:cover;
  pointer-events:none;
}

#start {
  position:fixed;
  top:0;
  left:0;
  width:100vw;
  height:100vh;
  display:flex;
  justify-content:center;
  align-items:center;
  background:black;
  color:white;
  font-size:36px;
  z-index:9999;
  cursor:pointer;
  pointer-events:auto;
}

#debug {
  position:fixed;
  top:10px;
  left:10px;
  color:lime;
  font-size:16px;
  z-index:10;
}
</style>
</head>
<body>

<div id="camera-container">
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas"></canvas>
</div>

<div id="debug">Initializing…</div>
<div id="start">TAP TO START</div>

<script>
const startBtn = document.getElementById("start");
const debug = document.getElementById("debug");
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");

canvas.width = window.innerWidth;
canvas.height = window.innerHeight;

let pose = null;
let lastHand = null;
let handVelocity = {x:0,y:0,z:0};
let lastTimestamp = null;

startBtn.addEventListener("click", async () => {
  // Fullscreen
  if (document.documentElement.requestFullscreen) {
    await document.documentElement.requestFullscreen();
  }
  startBtn.style.display = "none";
  await startBackCamera();
});

async function startBackCamera() {
  const devices = await navigator.mediaDevices.enumerateDevices();
  const backCamera = devices.find(d => d.kind === "videoinput" && d.label.toLowerCase().includes("back"));
  if(!backCamera){ debug.textContent="No back camera found!"; return; }

  const stream = await navigator.mediaDevices.getUserMedia({
    video:{deviceId:{exact:backCamera.deviceId}, width:1280, height:720},
    audio:false
  });
  video.srcObject = stream;

  pose = new Pose({locateFile:file=>`https://cdn.jsdelivr.net/npm/@mediapipe/pose/${file}`});
  pose.setOptions({modelComplexity:2,smoothLandmarks:true,minDetectionConfidence:0.5,minTrackingConfidence:0.5});
  pose.onResults(drawHandPredictor);

  async function loop(){
    if(video.readyState>=2) await pose.send({image:video});
    requestAnimationFrame(loop);
  }
  loop();
}

function drawHandPredictor(results){
  ctx.clearRect(0,0,canvas.width,canvas.height);

  if(!results.poseLandmarks){ debug.textContent="No body detected"; return; }

  debug.textContent="Tracking hand…";

  // Right hand (landmark 16 = wrist)
  const hand = results.poseLandmarks[16];
  const timestamp = performance.now()/1000;

  if(lastHand && lastTimestamp){
    const dt = timestamp - lastTimestamp;
    handVelocity.x = (hand.x - lastHand.x)/dt;
    handVelocity.y = (hand.y - lastHand.y)/dt;
    handVelocity.z = (hand.z - lastHand.z)/dt;
  }

  lastHand = hand;
  lastTimestamp = timestamp;

  // Draw hand
  const hx = hand.x*canvas.width;
  const hy = hand.y*canvas.height;
  ctx.beginPath();
  ctx.arc(hx,hy,10,0,Math.PI*2);
  ctx.fillStyle="red";
  ctx.fill();

  // Predicted trajectory
  const points = [];
  let px = hand.x, py = hand.y, pz = hand.z;
  let vx = handVelocity.x*0.05;
  let vy = handVelocity.y*0.05;
  const g = 0.0015;

  for(let i=0;i<60;i++){
    px += vx;
    py += vy;
    vy += g;
    points.push({x:px*canvas.width, y:py*canvas.height});
  }

  ctx.beginPath();
  for(let i=0;i<points.length;i++){
    const p = points[i];
    if(i===0) ctx.moveTo(p.x,p.y);
    else ctx.lineTo(p.x,p.y);
  }
  ctx.strokeStyle="lime";
  ctx.lineWidth=3;
  ctx.stroke();
}
</script>

</body>
</html>
